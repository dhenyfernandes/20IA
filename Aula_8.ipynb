{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# demais pacotes serão importados, mas vamos adicioná-los à medida que houver necessidade "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adquirindo os dados\n",
    "\n",
    "Vamos fazer a leitura de nossos dados utilizando Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('bases/forestfires.csv',sep=';')\n",
    "\n",
    "# Imprimindo as primeiras 5 linhas\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Diretamente, não temos informação do que se refere cada coluna de nosso dataset (a não ser as óbvias month e day, etc.). Por isso, temos um arquivo extra que nos informa o significado de cada coluna. Deixei ele junto com o dataset no github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos usar o método info() que é útil para obter uma rápida descrição dos dados, em particular o número de linhas, o tipo de cada \n",
    "# atributo e o número de valores não nulos\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos os atributos são ou int ou float, exceto month e day, que é object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 1\n",
    "Retorne uma Serie que informe os dados presentes na coluna month com a respectiva contagem de quantas vezes esse dado aparece no dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 2\n",
    "Percebemos que existem alguns meses com valores insignificantes. Remova do dataset as informações sobre os meses cujo valor é menor ou igual a 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O método describe() apresenta um resumo dos atributos numéricos\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As linhas count, mean, min e max são auto-explicativas. std mostra o desvio padrão (mede o quão disperso os valores são); 25%, 50% e 75% \n",
    "correspondem a percentis. Um percentil indica um valor abaixo do qual fica uma determinada porcentagem de observações de um grupo qualquer.\n",
    "\n",
    "Várias informações úteis nos são apresentadas quando utilizamos o método describe, mas as informações da coluna rain nos chamam a atenção. Podemos ver que em 75% dos casos não há chuvas, o que contribui para a expansão das queimadas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 3\n",
    "\n",
    "É possível notar que grande parte da distribuição do atributo meta se concentra numa área de até 6,58 ha (hectares -> 1ha == 10000m²). Use o método cut do Pandas para verificar os agrupamentos de área queimada. Crie uma nova coluna chamada area_cat e utilize os seguintes bins: 0-5, 5-10, 10-50, 50-100, >100. Leve em consideração o valor máximo presente nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop a coluna area_cat, pois no vamos utilizá-la mais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['area_cat'], inplace=True)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 4\n",
    "\n",
    "Use a combinação de Pandas + Matplotlib e retorne um histograma de cada uma das variáveis numéricas presente no dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 5\n",
    "\n",
    "Seguindo o mesmo procedimento, imprima os boxplots de cada coluna numérica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 6\n",
    "\n",
    "Calcule a correlação dos atributos descritores com o atributo meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resposta"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Muita coisa interessante a ser observada, mas o atributo rain novamente nos chama a atenção. Podemos confirmar nossa suposição anterior ao verificar que, quanto mais chuva, menos área queimada, e vice versa, já que a correlação é negativa entre esses dois atributos. Podemos afirmar também que quanto maior a temperatura, maior a área queimada. Entetando, os valores de correlação são muito baixos e parecem não explicar totalmente a causa das queimadas (o próprio artigo deixa claro que o problema é muito difícil). \n",
    "\n",
    "Há algumas razões:\n",
    "\n",
    "1) O dataset pode não ser suficientemente grande\n",
    "2) o dataset pode não ser bom\n",
    "3) faltam mais atributos para determinar o output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando os Dados para Algoritmos de Machine Learning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Poderíamos preparar os dados de modo manual, mas é mais interessante fazer isso escrevendo funções, por várias razões:\n",
    "    -> permite reproduzir essas transformações facilmente em qualquer dataset\n",
    "    -> gradualmente, você estará construindo sua biblioteca de funções de transformação que poderá ser reusada em projetos futuros.\n",
    "    -> poderemos usar essas funções em seus sistemas em produção para transformar novos dados antes de inseri-los em seu algoritmo\n",
    "    -> possibilidade de testar varias transformações e ver qual combinação funciona melhor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 7\n",
    "\n",
    "Crie duas variáveis: features e labels. A primeira vai conter todas as colunas, menos 'area'. A segunda, apenas a coluna área. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A maioria dos algoritmos de machine learning não funciona bem com características faltantes, então precisamos criar algumas funções para lidar com elas. \n",
    "\n",
    "Olhando nosso dataset, podemos observar alguns valores faltantes para o atributo temp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Para arrumar essa inconsistência, temos três opções:\n",
    "    -> eleminar as amostras correspondentes\n",
    "    -> eliminar todo o atributo\n",
    "    -> ajustar os dados para algum valor (zero, média, mediana, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.dropna(subset=[\"temp\"]) # opção 1\n",
    "# features.drop(\"temp\", axis=1) # opção 2\n",
    "# median = features[\"temp\"].median()\n",
    "# features[\"temp\"].fillna(median) # opção 3\n",
    "\n",
    "# lembrando que se a opção 3 for escolhida, precisamos tratar os dados tanto no conjunto de treino quanto no conjunto de teste"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Scikit_learn provê uma classe para cuidar dos valores faltantes: SimpleImputer. \n",
    "\n",
    "Primeiro, precisamos criar uma instância dessa classe, especificando que desejamos substituir cada valor faltante em cada atributo pela mediana desse atributo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Visto que a mediana só pode ser calculada em atributos numéricos, precisamos criar uma cópia dos dados sem os atributos do tipo objeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_num = features.drop(['month','day'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora precisamos ajustar a instância do Imputer aos dados de treinamento usando o método fit():\n",
    "imputer.fit(features_num)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "o imputer calculou a mediana de cada atributo e armazenou o resultado na variável statistics_. Originalmente, apenas o atributo temp possuia valores faltantes, mas não é possível garantir que não haverá valores faltantes quando o sistema entrar em produção. Então, é seguro aplicar imputer para todos os atributos numéricos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Agora, podemos usar esses valores obtidos para substituir valores faltantes pelas medianas aprendidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(features_num) # o resultado é um numpy.array. Precisamos converter para DataFrame\n",
    "features_transformed = pd.DataFrame(X, columns=features_num.columns)\n",
    "features_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos confirmar que os valores faltantes foram todos preenchidos\n",
    "features_transformed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization\n",
    "\n",
    "Com algumas poucas exceções, algoritmos de machine learning não performam bem quando os atributos numéricos possuem escalas diferentes.\n",
    "\n",
    "Existem duas abordagens conhecidas para trazer os atributos para uma mesma escala: min-max scaling e standardization. \n",
    "\n",
    "min-max scaling (normalization) consiste em subtratir o valor minimo e dividir pela subtração de max por min:\n",
    "\n",
    "\\begin{align}\n",
    "x = \\frac{x - min}{max - min}\n",
    "\\end{align}\n",
    "\n",
    "Já standardization consiste em subtrair a média(valores sempre terao média zero) e então dividir pela variância de modo que a distribuição resultante possua variância unitária. \n",
    "\n",
    "\\begin{align}\n",
    "x = \\frac{x - \\mu}{\\sigma}\n",
    "\\end{align}\n",
    "\n",
    "Diferentemente de normalization, standardization não leva os números a um intervalo específico, o que pode ser um problema para algoritmos específicos (por exemplo, redes neurais geralmente esperam um valor de entrada entre o intervalo 0-1). Entretanto, standardization é muito menos sensível à outiliers. \n",
    "\n",
    "Vamos usar as implementações disponíveis na Scikit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulando Atributos Textuais e Categóricos"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Anteriormente, deixamos de lado os atributos categóricos pelo motivo de serem um atributo de texto e, portanto, não é possível calcular sua mediana. Assim, precisamos converter esses rótulos em números. \n",
    "\n",
    "Scikit-Learn provê uma ferramenta para isso denominada OrdinalEncoder, que associa um valor para cada categoria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando apenas atributos categóricos\n",
    "features_cat = features[['month','day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "encoder = OrdinalEncoder()\n",
    "features_cat_1hot = encoder.fit_transform(features_cat)\n",
    "features_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines de Transformação"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Como é possível observar, existem muitos passos no processo de transformação de dados que precisam ser executados na ordem correta. Para isso, Scikit-Learn provê uma classe que auxilia essas sequências de transformações: Pipeline. \n",
    "\n",
    "Para os atributos numéricos, podemos escrever um pipeline da seguinte maneira:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer',SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "])\n",
    "features_num_tr = num_pipeline.fit_transform(features_num)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Mas ainda precisamos lidar com os atributos categóricos e, além disso, juntar todo o processo de transformação num único pipeline para obter um único resultado de todos os dados tratados que, em teoria, estariam prontos para um algoritmo de machine learning (obviamente, antes precisariam ser divididos em conjunto de treino e teste - veremos isso adiante). \n",
    "\n",
    "Para criar um pipeline que trate tanto atributos numéricos quanto categóricos, podemos usar FeatureUnion. Antes, no entanto, vamos criar uma classe para separar os atributos numéricos e categóricos de nosso dataset original "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 8\n",
    "\n",
    "Complete a classe DataFrameSelector que herde BaseEstimator e TransformerMixin (from sklearn.base import BaseEstimator, TransformerMixin) que implemente 3 métodos:\n",
    "\n",
    "> __init__: recebe o nome dos atributos\n",
    "\n",
    "> fit: retorna self\n",
    "\n",
    "> transform: aciona o atributo values do DataFrame[atributos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resposta\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class DataFrameSelector(__,__):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = ___\n",
    "    def fit(self, X, y=None):\n",
    "        return ___\n",
    "    def transform(self, X):\n",
    "        return ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe DataFrameSelector irá separar o dataframe de acordo com o conteúdo da variável attribute_names, retornando apenas aquelas que são categórias ou aquelas que são numéricas. Essa classe herda os métodos BaseEstimator e TransformerMixin.\n",
    "\n",
    "O primeiro é responsável por implementar os métodos get_params() e set_params() que são extremamente úteis quando vamos executar um gridsearch, por exemplo. Com essa classe podemos acessar os valores que estão definidos dentro do método __init__, bem como atribuir novos valores apra ele. \n",
    "\n",
    "Já TransformerMixin tras implementado o método fit_transform(), que nada mais é do que aplicar o método fit() e, logo em seguida, o método transform.\n",
    "\n",
    "Esse link pode ajudar a esclarecer melhor as coisas, caso ainda tenham dúvidas: [link](https://github.com/ageron/handson-ml/issues/391)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizacao usando Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import OrdinalEncoder #skl 0.20>\n",
    "from sklearn.preprocessing import StandardScaler # MinMaxScaler\n",
    "num_attribs = list(features_num)\n",
    "cat_attribs = ['month','day']\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(num_attribs)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(cat_attribs)),\n",
    "    ('categorial_encoder', OrdinalEncoder())\n",
    "    ])\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"cat_pipeline\", cat_pipeline),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_prepared = full_pipeline.fit_transform(features) #retorna um numpy.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora temos nossos dados preparados.\n",
    "features_prepared[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_prepared\n",
    "y = labels.values\n",
    "split_test_size = 0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = linear_model.LinearRegression()\n",
    "lr_model.fit(X_train, y_train.ravel()) # flatten arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.sqrt(mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
